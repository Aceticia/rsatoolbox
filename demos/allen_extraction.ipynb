{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started exercise for RSA3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In these three exercises you will get an introduction to the functionality of the new pyRSA-toolbox for inferring the underlying model representation based on measured data. Generally we assume that there is a true underlying representation, which is captured by our model. The measurement process like fMRI will lead to a distorted view of that representation, which we may or may not include into our analysis as an explicit measurement model.  \n",
    "\n",
    "For illustration these exercises use simulated RDMs from the paper \"Inferring brain-computational mechanisms with models of activity measurements\" by Kriegeskorte & Diedrichsen (2016). Ground truth RDMs are here simulated based on the layers of Alexnet--the deep neural network model, which sparked the interest in deep learning. Simulated data rdms were generated as follows: First, voxel responses were generated by randomly selecting locations within the layer and modelling their response as a local average of the feature values. Then, noise was added to those voxel responses and RDMs were computed from these noisy responses. As model predictions to compare to, we use noise-free RDMs generated for each layer, by applying different amounts of smoothing and averaging to the layer representation. \n",
    "\n",
    "Our overall aim in this setting is to infer which representation the data rdms were based on, i.e. which layer was used for generating the data. Towards this aim we will make three steps:\n",
    "\n",
    "In *Exercise 1*, we will load the data, convert them into the formats used in the toolbox and have a first exploratory look at the data.\n",
    "\n",
    "In *Exercise 2*, we will compare the RDMs based on the undistorted representations to the simulated data RDMs. This is the classical and simplest approach and already allows us to perform model comparisons and the general evaluation of model-RDMs. This approach uses *fixed models*, i.e. each model predicts a single fixed RDM. We will see that this does not allow us to correctly infer the underlying representation though, because the measurement process distorts the RDMs too much.\n",
    "\n",
    "In *Exercise 3*, we will apply *flexible models*. This means that each model predicts a distribution of RDMs. In the present context this means that the model is flexible in which measurement model is applied to explain the data. To evaluate such flexible models additional cross-validation is necessary, which we also discuss in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Data and RDM handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rsa3 in /Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages (3.0.0.post20201106)\n",
      "Requirement already satisfied: h5py in /Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages (from rsa3) (2.10.0)\n",
      "Requirement already satisfied: scikit-image in /Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages (from rsa3) (0.16.2)\n",
      "Requirement already satisfied: tqdm in /Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages (from rsa3) (4.56.0)\n",
      "Requirement already satisfied: matplotlib in /Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages (from rsa3) (3.3.3)\n",
      "Requirement already satisfied: numpy in /Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages (from rsa3) (1.18.5)\n",
      "Requirement already satisfied: coverage in /Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages (from rsa3) (5.3.1)\n",
      "Requirement already satisfied: scipy in /Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages (from rsa3) (1.6.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages (from rsa3) (0.24.1)\n",
      "Requirement already satisfied: joblib in /Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages (from rsa3) (1.0.0)\n",
      "Requirement already satisfied: six in /Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages (from h5py->rsa3) (1.15.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages (from matplotlib->rsa3) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages (from matplotlib->rsa3) (8.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages (from matplotlib->rsa3) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages (from matplotlib->rsa3) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages (from matplotlib->rsa3) (2.4.7)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages (from scikit-image->rsa3) (2.9.0)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages (from scikit-image->rsa3) (1.1.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages (from scikit-image->rsa3) (2.5)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages (from networkx>=2.0->scikit-image->rsa3) (4.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages (from scikit-learn->rsa3) (2.1.0)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyrsa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0deeaab1f40e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install rsa3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyrsa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyrsa'"
     ]
    }
   ],
   "source": [
    "!pip install rsa3\n",
    "import pyrsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import io \n",
    "import matplotlib.pyplot as plt\n",
    "import pyrsa\n",
    "import pickle as pk\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model RDMs\n",
    "Here the models are different layers of Alexnet.\n",
    "For each layer, different models of how the fMRI voxels sample the neurons are being considered.\n",
    "\n",
    "The simulated data were generated in Matlab (Kriegeskorte & Diedrichsen 2016). Thus, we load the Matlab files in .mat format.\n",
    "\n",
    "For each model-RDM, we obtain the RDM itself, a model name, and a measurement model name. The model name specifies the layer used to generate the RDM. The measurement model name specifies the applied distortions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allensdk.core.brain_observatory_cache import BrainObservatoryCache\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "boc = BrainObservatoryCache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(456, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_areas = ['VISal', 'VISam', 'VISl', 'VISp', 'VISpm', 'VISrl']\n",
    "exps = boc.get_experiment_containers(targeted_structures=visual_areas)\n",
    "data_set = pd.DataFrame(exps)\n",
    "\n",
    "data_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>imaging_depth</th>\n",
       "      <th>targeted_structure</th>\n",
       "      <th>cre_line</th>\n",
       "      <th>reporter_line</th>\n",
       "      <th>donor_name</th>\n",
       "      <th>specimen_name</th>\n",
       "      <th>tags</th>\n",
       "      <th>failed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>660492886</td>\n",
       "      <td>275</td>\n",
       "      <td>VISal</td>\n",
       "      <td>Sst-IRES-Cre</td>\n",
       "      <td>Ai148(TIT2L-GC6f-ICL-tTA2)</td>\n",
       "      <td>306500</td>\n",
       "      <td>Sst-IRES-Cre;Ai148(CAM)-306500</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>651773425</td>\n",
       "      <td>375</td>\n",
       "      <td>VISl</td>\n",
       "      <td>Fezf2-CreER</td>\n",
       "      <td>Ai148(TIT2L-GC6f-ICL-tTA2)</td>\n",
       "      <td>351315</td>\n",
       "      <td>Fezf2-CreER;Ai148-351315</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>546328009</td>\n",
       "      <td>350</td>\n",
       "      <td>VISal</td>\n",
       "      <td>Nr5a1-Cre</td>\n",
       "      <td>Ai93(TITL-GCaMP6f)</td>\n",
       "      <td>261969</td>\n",
       "      <td>Nr5a1-Cre;Camk2a-tTA;Ai93-261969</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>661437138</td>\n",
       "      <td>175</td>\n",
       "      <td>VISp</td>\n",
       "      <td>Slc17a7-IRES2-Cre</td>\n",
       "      <td>Ai93(TITL-GCaMP6f)</td>\n",
       "      <td>365251</td>\n",
       "      <td>Slc17a7-IRES2-Cre;Camk2a-tTA;Ai93-365251</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>569792815</td>\n",
       "      <td>375</td>\n",
       "      <td>VISam</td>\n",
       "      <td>Emx1-IRES-Cre</td>\n",
       "      <td>Ai93(TITL-GCaMP6f)</td>\n",
       "      <td>284669</td>\n",
       "      <td>Emx1-IRES-Cre;Camk2a-tTA;Ai93-284669</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  imaging_depth targeted_structure           cre_line  \\\n",
       "0  660492886            275              VISal       Sst-IRES-Cre   \n",
       "1  651773425            375               VISl        Fezf2-CreER   \n",
       "2  546328009            350              VISal          Nr5a1-Cre   \n",
       "3  661437138            175               VISp  Slc17a7-IRES2-Cre   \n",
       "4  569792815            375              VISam      Emx1-IRES-Cre   \n",
       "\n",
       "                reporter_line donor_name  \\\n",
       "0  Ai148(TIT2L-GC6f-ICL-tTA2)     306500   \n",
       "1  Ai148(TIT2L-GC6f-ICL-tTA2)     351315   \n",
       "2          Ai93(TITL-GCaMP6f)     261969   \n",
       "3          Ai93(TITL-GCaMP6f)     365251   \n",
       "4          Ai93(TITL-GCaMP6f)     284669   \n",
       "\n",
       "                              specimen_name tags  failed  \n",
       "0            Sst-IRES-Cre;Ai148(CAM)-306500   []   False  \n",
       "1                  Fezf2-CreER;Ai148-351315   []   False  \n",
       "2          Nr5a1-Cre;Camk2a-tTA;Ai93-261969   []   False  \n",
       "3  Slc17a7-IRES2-Cre;Camk2a-tTA;Ai93-365251   []   False  \n",
       "4      Emx1-IRES-Cre;Camk2a-tTA;Ai93-284669   []   False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(456, 11)\n",
      "(76, 11)\n",
      "(380, 11)\n",
      "(380, 11)\n",
      "(1368, 11)\n",
      "(456, 11)\n",
      "(456, 11)\n",
      "(456, 11)\n",
      "(1368, 11)\n",
      "(456, 11)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "exp = []\n",
    "stms = ['drifting_gratings',\n",
    "        'locally_sparse_noise',\n",
    "        'locally_sparse_noise_4deg',\n",
    "        'locally_sparse_noise_8deg',\n",
    "        'natural_movie_one',\n",
    "        'natural_movie_three',\n",
    "        'natural_movie_two',\n",
    "        'natural_scenes',\n",
    "        'spontaneous',\n",
    "        'static_gratings']\n",
    "for stm in stms:\n",
    "    os.system(\"cd allendata; mkdir \"+stm+\"; cd ..\")\n",
    "    exp_obj_info = pd.DataFrame(boc.get_ophys_experiments(stimuli=[stm]))\n",
    "    print(exp_obj_info.shape)\n",
    "    exp_obj_info.to_csv(\"allendata/\"+stm+\"/exp_info.csv\")\n",
    "    exp.append(exp_obj_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>imaging_depth</th>\n",
       "      <th>targeted_structure</th>\n",
       "      <th>cre_line</th>\n",
       "      <th>reporter_line</th>\n",
       "      <th>acquisition_age_days</th>\n",
       "      <th>experiment_container_id</th>\n",
       "      <th>session_type</th>\n",
       "      <th>donor_name</th>\n",
       "      <th>specimen_name</th>\n",
       "      <th>fail_eye_tracking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>509729072</td>\n",
       "      <td>275</td>\n",
       "      <td>VISp</td>\n",
       "      <td>Rorb-IRES2-Cre</td>\n",
       "      <td>Ai93(TITL-GCaMP6f)</td>\n",
       "      <td>102</td>\n",
       "      <td>511510675</td>\n",
       "      <td>three_session_C</td>\n",
       "      <td>228786</td>\n",
       "      <td>Rorb-IRES2-Cre;Camk2a-tTA;Ai93-228786</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>502974807</td>\n",
       "      <td>275</td>\n",
       "      <td>VISp</td>\n",
       "      <td>Cux2-CreERT2</td>\n",
       "      <td>Ai93(TITL-GCaMP6f)</td>\n",
       "      <td>98</td>\n",
       "      <td>511510699</td>\n",
       "      <td>three_session_C</td>\n",
       "      <td>225037</td>\n",
       "      <td>Cux2-CreERT2;Camk2a-tTA;Ai93-225037</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>510524416</td>\n",
       "      <td>275</td>\n",
       "      <td>VISal</td>\n",
       "      <td>Rorb-IRES2-Cre</td>\n",
       "      <td>Ai93(TITL-GCaMP6f)</td>\n",
       "      <td>82</td>\n",
       "      <td>511500480</td>\n",
       "      <td>three_session_C</td>\n",
       "      <td>232623</td>\n",
       "      <td>Rorb-IRES2-Cre;Camk2a-tTA;Ai93-232623</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>539000397</td>\n",
       "      <td>350</td>\n",
       "      <td>VISp</td>\n",
       "      <td>Nr5a1-Cre</td>\n",
       "      <td>Ai93(TITL-GCaMP6f)</td>\n",
       "      <td>79</td>\n",
       "      <td>538803515</td>\n",
       "      <td>three_session_C</td>\n",
       "      <td>257786</td>\n",
       "      <td>Nr5a1-Cre;Camk2a-tTA;Ai93-257786</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>502352946</td>\n",
       "      <td>175</td>\n",
       "      <td>VISl</td>\n",
       "      <td>Cux2-CreERT2</td>\n",
       "      <td>Ai93(TITL-GCaMP6f)</td>\n",
       "      <td>115</td>\n",
       "      <td>511510640</td>\n",
       "      <td>three_session_C</td>\n",
       "      <td>222426</td>\n",
       "      <td>Cux2-CreERT2;Camk2a-tTA;Ai93-222426</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  imaging_depth targeted_structure        cre_line  \\\n",
       "0  509729072            275               VISp  Rorb-IRES2-Cre   \n",
       "1  502974807            275               VISp    Cux2-CreERT2   \n",
       "2  510524416            275              VISal  Rorb-IRES2-Cre   \n",
       "3  539000397            350               VISp       Nr5a1-Cre   \n",
       "4  502352946            175               VISl    Cux2-CreERT2   \n",
       "\n",
       "        reporter_line  acquisition_age_days  experiment_container_id  \\\n",
       "0  Ai93(TITL-GCaMP6f)                   102                511510675   \n",
       "1  Ai93(TITL-GCaMP6f)                    98                511510699   \n",
       "2  Ai93(TITL-GCaMP6f)                    82                511500480   \n",
       "3  Ai93(TITL-GCaMP6f)                    79                538803515   \n",
       "4  Ai93(TITL-GCaMP6f)                   115                511510640   \n",
       "\n",
       "      session_type donor_name                          specimen_name  \\\n",
       "0  three_session_C     228786  Rorb-IRES2-Cre;Camk2a-tTA;Ai93-228786   \n",
       "1  three_session_C     225037    Cux2-CreERT2;Camk2a-tTA;Ai93-225037   \n",
       "2  three_session_C     232623  Rorb-IRES2-Cre;Camk2a-tTA;Ai93-232623   \n",
       "3  three_session_C     257786       Nr5a1-Cre;Camk2a-tTA;Ai93-257786   \n",
       "4  three_session_C     222426    Cux2-CreERT2;Camk2a-tTA;Ai93-222426   \n",
       "\n",
       "   fail_eye_tracking  \n",
       "0              False  \n",
       "1               True  \n",
       "2               True  \n",
       "3              False  \n",
       "4               True  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>imaging_depth</th>\n",
       "      <th>targeted_structure</th>\n",
       "      <th>cre_line</th>\n",
       "      <th>reporter_line</th>\n",
       "      <th>acquisition_age_days</th>\n",
       "      <th>experiment_container_id</th>\n",
       "      <th>session_type</th>\n",
       "      <th>donor_name</th>\n",
       "      <th>specimen_name</th>\n",
       "      <th>fail_eye_tracking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>588535615</td>\n",
       "      <td>275</td>\n",
       "      <td>VISal</td>\n",
       "      <td>Sst-IRES-Cre</td>\n",
       "      <td>Ai148(TIT2L-GC6f-ICL-tTA2)</td>\n",
       "      <td>99</td>\n",
       "      <td>660492886</td>\n",
       "      <td>three_session_C2</td>\n",
       "      <td>306500</td>\n",
       "      <td>Sst-IRES-Cre;Ai148(CAM)-306500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>590109296</td>\n",
       "      <td>275</td>\n",
       "      <td>VISal</td>\n",
       "      <td>Sst-IRES-Cre</td>\n",
       "      <td>Ai148(TIT2L-GC6f-ICL-tTA2)</td>\n",
       "      <td>106</td>\n",
       "      <td>660492886</td>\n",
       "      <td>three_session_B</td>\n",
       "      <td>306500</td>\n",
       "      <td>Sst-IRES-Cre;Ai148(CAM)-306500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589441079</td>\n",
       "      <td>275</td>\n",
       "      <td>VISal</td>\n",
       "      <td>Sst-IRES-Cre</td>\n",
       "      <td>Ai148(TIT2L-GC6f-ICL-tTA2)</td>\n",
       "      <td>104</td>\n",
       "      <td>660492886</td>\n",
       "      <td>three_session_A</td>\n",
       "      <td>306500</td>\n",
       "      <td>Sst-IRES-Cre;Ai148(CAM)-306500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  imaging_depth targeted_structure      cre_line  \\\n",
       "0  588535615            275              VISal  Sst-IRES-Cre   \n",
       "1  590109296            275              VISal  Sst-IRES-Cre   \n",
       "2  589441079            275              VISal  Sst-IRES-Cre   \n",
       "\n",
       "                reporter_line  acquisition_age_days  experiment_container_id  \\\n",
       "0  Ai148(TIT2L-GC6f-ICL-tTA2)                    99                660492886   \n",
       "1  Ai148(TIT2L-GC6f-ICL-tTA2)                   106                660492886   \n",
       "2  Ai148(TIT2L-GC6f-ICL-tTA2)                   104                660492886   \n",
       "\n",
       "       session_type donor_name                   specimen_name  \\\n",
       "0  three_session_C2     306500  Sst-IRES-Cre;Ai148(CAM)-306500   \n",
       "1   three_session_B     306500  Sst-IRES-Cre;Ai148(CAM)-306500   \n",
       "2   three_session_A     306500  Sst-IRES-Cre;Ai148(CAM)-306500   \n",
       "\n",
       "   fail_eye_tracking  \n",
       "0              False  \n",
       "1              False  \n",
       "2              False  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_id = data_set['id'][0]\n",
    "exp_obj_info = pd.DataFrame(boc.get_ophys_experiments(experiment_container_ids=[exp_id]))\n",
    "# exp_ses_id = boc.get_ophys_experiments(experiment_container_ids=[exp_id], stimuli=['natural_scenes'])[0]['id']\n",
    "# ses_obj = boc.get_ophys_experiment_data(ophys_experiment_id=exp_ses_id)    \n",
    "# ns = boc.get_ophys_experiment_analysis(ophys_experiment_id=exp_ses_id, stimulus_type='natural_scenes')\n",
    "\n",
    "exp_obj_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 590109296,\n",
       "  'imaging_depth': 275,\n",
       "  'targeted_structure': 'VISal',\n",
       "  'cre_line': 'Sst-IRES-Cre',\n",
       "  'reporter_line': 'Ai148(TIT2L-GC6f-ICL-tTA2)',\n",
       "  'acquisition_age_days': 106,\n",
       "  'experiment_container_id': 660492886,\n",
       "  'session_type': 'three_session_B',\n",
       "  'donor_name': '306500',\n",
       "  'specimen_name': 'Sst-IRES-Cre;Ai148(CAM)-306500',\n",
       "  'fail_eye_tracking': False}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_ses_ids = [x for x in boc.get_ophys_experiments(experiment_container_ids=[exp_id],stimuli=['natural_scenes'])]\n",
    "exp_ses_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<allensdk.core.brain_observatory_nwb_data_set.BrainObservatoryNwbDataSet at 0x7f8bd0eb4510>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_ses_id = exp_ses_ids[0]['id']\n",
    "ses_obj = boc.get_ophys_experiment_data(ophys_experiment_id=exp_ses_id) \n",
    "ses_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 113831)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, dff_traces = ses_obj.get_dff_traces()\n",
    "dff_traces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   9.88188,    9.91513,    9.94839, ..., 3795.1064 , 3795.13966,\n",
       "       3795.17291])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-03 12:36:57,516 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/516244761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(267, 105716)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_ses_id = 509729072\n",
    "ses_obj = boc.get_ophys_experiment_data(ophys_experiment_id=exp_ses_id) \n",
    "a, dff_traces = ses_obj.get_dff_traces()\n",
    "dff_traces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages/allensdk/brain_observatory/natural_scenes.py:154: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  peak.scene_ns[nc] = nsp\n",
      "/Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages/allensdk/brain_observatory/natural_scenes.py:157: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  peak.peak_dff_ns[nc] = self.response[nsp + 1, nc, 0]\n",
      "/Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages/allensdk/brain_observatory/natural_scenes.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  (_, peak.ptest_ns[nc]) = st.f_oneway(*groups)\n",
      "/Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages/allensdk/brain_observatory/natural_scenes.py:178: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  np.argmax(test) - self.interlength) / self.acquisition_rate\n",
      "/Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scene_ns</th>\n",
       "      <th>reliability_ns</th>\n",
       "      <th>peak_dff_ns</th>\n",
       "      <th>ptest_ns</th>\n",
       "      <th>p_run_ns</th>\n",
       "      <th>run_modulation_ns</th>\n",
       "      <th>time_to_peak_ns</th>\n",
       "      <th>cell_specimen_id</th>\n",
       "      <th>image_selectivity_ns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00334432</td>\n",
       "      <td>19.5795</td>\n",
       "      <td>0.0270341</td>\n",
       "      <td>0.0214276</td>\n",
       "      <td>1.06672</td>\n",
       "      <td>0.3325</td>\n",
       "      <td>599564642</td>\n",
       "      <td>0.271153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0992196</td>\n",
       "      <td>5.21349</td>\n",
       "      <td>3.98115e-06</td>\n",
       "      <td>0.578964</td>\n",
       "      <td>-0.176441</td>\n",
       "      <td>0.3325</td>\n",
       "      <td>599564702</td>\n",
       "      <td>0.158441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114</td>\n",
       "      <td>0.0602141</td>\n",
       "      <td>5.53101</td>\n",
       "      <td>1.85331e-05</td>\n",
       "      <td>0.213904</td>\n",
       "      <td>0.548526</td>\n",
       "      <td>0.43225</td>\n",
       "      <td>599564634</td>\n",
       "      <td>0.31539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.164042</td>\n",
       "      <td>14.9942</td>\n",
       "      <td>2.36489e-10</td>\n",
       "      <td>0.0667954</td>\n",
       "      <td>0.604232</td>\n",
       "      <td>0.29925</td>\n",
       "      <td>599564624</td>\n",
       "      <td>0.527322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.471215</td>\n",
       "      <td>23.7377</td>\n",
       "      <td>5.58788e-151</td>\n",
       "      <td>0.0233042</td>\n",
       "      <td>0.427992</td>\n",
       "      <td>0.23275</td>\n",
       "      <td>599564655</td>\n",
       "      <td>0.707932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  scene_ns reliability_ns peak_dff_ns      ptest_ns   p_run_ns  \\\n",
       "0        0     0.00334432     19.5795     0.0270341  0.0214276   \n",
       "1        5      0.0992196     5.21349   3.98115e-06   0.578964   \n",
       "2      114      0.0602141     5.53101   1.85331e-05   0.213904   \n",
       "3        5       0.164042     14.9942   2.36489e-10  0.0667954   \n",
       "4        5       0.471215     23.7377  5.58788e-151  0.0233042   \n",
       "\n",
       "  run_modulation_ns time_to_peak_ns  cell_specimen_id image_selectivity_ns  \n",
       "0           1.06672          0.3325         599564642             0.271153  \n",
       "1         -0.176441          0.3325         599564702             0.158441  \n",
       "2          0.548526         0.43225         599564634              0.31539  \n",
       "3          0.604232         0.29925         599564624             0.527322  \n",
       "4          0.427992         0.23275         599564655             0.707932  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from allensdk.brain_observatory.natural_scenes import NaturalScenes\n",
    "\n",
    "ns = NaturalScenes(ses_obj)\n",
    "ns.peak.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "def plot_stimulus_table(stim_table, title):\n",
    "    fstart = stim_table.start.min()\n",
    "    fend = stim_table.end.max()\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,1))\n",
    "    ax = fig.gca()\n",
    "    for i, trial in stim_table.iterrows():    \n",
    "        x1 = float(trial.start - fstart) / (fend - fstart)\n",
    "        x2 = float(trial.end - fstart) / (fend - fstart)            \n",
    "        ax.add_patch(patches.Rectangle((x1, 0.0), x2 - x1, 1.0, color='r'))\n",
    "    ax.set_xticks((0,1))\n",
    "    ax.set_xticklabels((fstart, fend))\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_set = boc.get_ophys_experiment_data(501498760)\n",
    "\n",
    "# the natural scenes stimulus table describes when each scene is on the screen\n",
    "stim_table = data_set.get_stimulus_table('natural_scenes')\n",
    "\n",
    "scene_nums = [2]\n",
    "\n",
    "# build up a mask of trials for which one of a list of scenes is visible\n",
    "trial_mask = stim_table.frame == -2\n",
    "for scene in scene_nums:\n",
    "    trial_mask |= (stim_table.frame == scene)\n",
    "stim_table = stim_table[trial_mask]\n",
    "\n",
    "# plot the trials\n",
    "plot_stimulus_table(stim_table, \"scenes %s \" % scene_nums)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>111</td>\n",
       "      <td>16126</td>\n",
       "      <td>16133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89</td>\n",
       "      <td>16133</td>\n",
       "      <td>16140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>113</td>\n",
       "      <td>16141</td>\n",
       "      <td>16148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>16148</td>\n",
       "      <td>16155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117</td>\n",
       "      <td>16156</td>\n",
       "      <td>16163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5945</th>\n",
       "      <td>44</td>\n",
       "      <td>96274</td>\n",
       "      <td>96281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5946</th>\n",
       "      <td>49</td>\n",
       "      <td>96281</td>\n",
       "      <td>96288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5947</th>\n",
       "      <td>105</td>\n",
       "      <td>96289</td>\n",
       "      <td>96296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5948</th>\n",
       "      <td>6</td>\n",
       "      <td>96296</td>\n",
       "      <td>96303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5949</th>\n",
       "      <td>98</td>\n",
       "      <td>96304</td>\n",
       "      <td>96311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5950 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      frame  start    end\n",
       "0       111  16126  16133\n",
       "1        89  16133  16140\n",
       "2       113  16141  16148\n",
       "3         6  16148  16155\n",
       "4       117  16156  16163\n",
       "...     ...    ...    ...\n",
       "5945     44  96274  96281\n",
       "5946     49  96281  96288\n",
       "5947    105  96289  96296\n",
       "5948      6  96296  96303\n",
       "5949     98  96304  96311\n",
       "\n",
       "[5950 rows x 3 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stim_table = data_set.get_stimulus_table('natural_scenes')\n",
    "stim_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(212, 9)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns = NaturalScenes(data_set)\n",
    "ns.peak.head()\n",
    "ns.peak.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644910997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 08:44:35,969 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/644919722\n",
      "2021-03-01 08:49:37,043 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/588865193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588656922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 08:52:11,715 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/563826648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563500510\n",
      "601362437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 09:00:04,943 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/601421428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647148822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 09:02:49,540 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/647295526\n",
      "2021-03-01 09:03:51,393 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/516243670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504593468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 09:13:07,542 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/516243736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506156402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 09:15:13,289 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/531327011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531124922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 09:20:43,057 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/516244121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506520696\n",
      "565293865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 09:24:08,360 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/565837494\n",
      "2021-03-01 09:25:20,500 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/653178856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653173685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 09:38:35,602 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/672690081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "672674656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 09:40:54,673 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/583443582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "583149151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 09:42:28,423 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/649334201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649317434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 09:47:44,488 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/583357879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "583137106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 09:48:16,517 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/552464031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552324309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 09:51:17,948 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/570994408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570909395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 09:55:21,503 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/601480710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601328878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 10:00:13,082 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/516238768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505696248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 10:05:35,558 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/572495463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "571912779\n",
      "653332425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 10:07:18,787 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/653336828\n",
      "2021-03-01 10:10:36,024 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/605601278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605465843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 10:12:09,696 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/663750453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663479950\n",
      "658816608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 10:15:55,614 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/659411579\n",
      "2021-03-01 10:16:36,137 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/639456535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "639443233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 10:17:28,463 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/671180246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "671162646\n",
      "571099190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 10:20:34,275 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/571177249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528574532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 10:23:35,219 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/528855373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511458874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 10:29:20,673 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/516803214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562660121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 10:31:14,368 allensdk.api.api.retrieve_file_over_http INFO     Downloading URL: http://api.brain-map.org/api/v2/well_known_file_download/562771934\n"
     ]
    }
   ],
   "source": [
    "from allensdk.brain_observatory.natural_scenes import NaturalScenes\n",
    "info = pd.read_csv(\"allendata/natural_scenes/exp_info.csv\")\n",
    "for i in info['id'][232:]:\n",
    "    print(i)\n",
    "    data_set = boc.get_ophys_experiment_data(i)\n",
    "    stim_table = data_set.get_stimulus_table('natural_scenes')\n",
    "    ns = NaturalScenes(data_set)\n",
    "    np.save(\"allendata/natural_scenes/sweep/\"+str(i)+\".npy\",np.array(ns.sweep_response))\n",
    "    stim_table.to_csv(\"allendata/natural_scenes/trialinfo/\"+str(i)+\".csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BETTER EXTRACTION\n",
    "\n",
    "from allensdk.brain_observatory.natural_scenes import NaturalScenes\n",
    "info = pd.read_csv(\"allendata/natural_scenes/exp_info.csv\")\n",
    "for i in info['id']:\n",
    "    print(i)\n",
    "    if not path.exists(\"allendata/natural_scenes/response3d/\"+str(i)+\".npy\"):\n",
    "        data_set = boc.get_ophys_experiment_data(i)\n",
    "        stim_table = data_set.get_stimulus_table('natural_scenes')\n",
    "        ns = NaturalScenes(data_set)\n",
    "        data = np.array(ns.sweep_response)\n",
    "        data = truc(data)\n",
    "        np.save(\"allendata/natural_scenes/response3d/\"+str(i)+\".npy\",data)\n",
    "        np.save(\"allendata/natural_scenes/response2d/\"+str(i)+\".npy\",seq2mean(data))\n",
    "    stim_table.to_csv(\"allendata/natural_scenes/trialinfo/\"+str(i)+\".csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npy2tempdim(d):\n",
    "    dnew = np.ndarray((d.shape[0],d.shape[1],d[0][0].shape[0]))\n",
    "    for i in np.arange(d.shape[0]):\n",
    "        for j in np.arange(d.shape[1]):\n",
    "            dnew[i,j,:] = np.float64(d[i][j])\n",
    "    return dnew\n",
    "\n",
    "def truc(d):\n",
    "    dcopy = d.copy()\n",
    "    for i,r in enumerate(d):\n",
    "        for j,c in enumerate(r):\n",
    "            dcopy[i,j] = c[0:8]\n",
    "    return npy2tempdim(dcopy[:,:-1]) \n",
    "\n",
    "def seq2mean(d):\n",
    "    return d.mean(axis=2)\n",
    "\n",
    "def trial2fold(labels, verbose=False):\n",
    "    dic = {}\n",
    "    fold = []\n",
    "    for l in labels:\n",
    "        if l in dic:\n",
    "            dic[l] += 1\n",
    "        else:\n",
    "            dic[l] = 0\n",
    "        fold.append(dic[l])\n",
    "    if verbose:\n",
    "        return np.array(fold),dic\n",
    "    else:\n",
    "        return np.array(fold)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500964514\n",
      "501559087\n",
      "501567237\n",
      "501794235\n",
      "501889084\n",
      "502254330\n",
      "502368172\n",
      "502382906\n",
      "503772253\n",
      "503866276\n",
      "504508104\n",
      "504625475\n",
      "505314372\n",
      "506278598\n",
      "506520703\n",
      "507304910\n",
      "508378520\n",
      "508546728\n",
      "509962140\n",
      "510166410\n",
      "510345479\n",
      "510517609\n",
      "510698988\n",
      "510699005\n",
      "510705057\n",
      "510706209\n",
      "510712856\n",
      "510938357\n",
      "511458599\n",
      "512124564\n",
      "512149367\n",
      "528480613\n",
      "529487172\n",
      "531342486\n",
      "539291372\n",
      "541048140\n",
      "541206592\n",
      "544507627\n",
      "547573479\n",
      "548227481\n",
      "550197614\n",
      "553000583\n",
      "553012563\n",
      "554021353\n",
      "555018432\n",
      "555257822\n",
      "555813683\n",
      "556338149\n",
      "556919719\n",
      "557182484\n",
      "557420967\n",
      "557956194\n",
      "560806119\n",
      "561528269\n",
      "561994407\n",
      "562220433\n",
      "562222842\n",
      "562296530\n",
      "566716486\n",
      "567446262\n",
      "567734055\n",
      "568753147\n",
      "568775666\n",
      "568796683\n",
      "569407590\n",
      "569722788\n",
      "569810774\n",
      "570006683\n",
      "570059563\n",
      "570080979\n",
      "570472763\n",
      "572489757\n",
      "572499364\n",
      "574415929\n",
      "574685634\n",
      "574824922\n",
      "575890352\n",
      "578220711\n",
      "578260272\n",
      "579966129\n",
      "579968437\n",
      "580124131\n",
      "580570243\n",
      "580631157\n",
      "581676766\n",
      "583296652\n",
      "584930390\n",
      "584983527\n",
      "585953317\n",
      "589098031\n",
      "589637407\n",
      "590109296\n",
      "591414748\n",
      "591563201\n",
      "591640135\n",
      "591823992\n",
      "593240301\n",
      "593389688\n",
      "593646972\n",
      "593695648\n",
      "593887846\n",
      "594314285\n",
      "595899822\n",
      "595904738\n",
      "596824582\n",
      "598905882\n",
      "599420257\n",
      "601260046\n",
      "601338233\n",
      "602170460\n",
      "603226974\n",
      "603516552\n",
      "603552279\n",
      "603863146\n",
      "605683787\n",
      "605913519\n",
      "606031380\n",
      "606151117\n",
      "606873744\n",
      "606960609\n",
      "612566550\n",
      "613062511\n",
      "613074493\n",
      "614535829\n",
      "614851823\n",
      "617395439\n",
      "629789161\n",
      "636924826\n",
      "636930038\n",
      "637123467\n",
      "637670417\n",
      "638754323\n",
      "639117180\n",
      "639252499\n",
      "639437387\n",
      "642651898\n",
      "645035917\n",
      "645040965\n",
      "645687787\n",
      "645695159\n",
      "646291324\n",
      "648389302\n",
      "648644110\n",
      "649398482\n",
      "650509372\n",
      "650510708\n",
      "652336350\n",
      "652337551\n",
      "652345569\n",
      "652988777\n",
      "652989705\n",
      "652990651\n",
      "653122445\n",
      "653922961\n",
      "653924226\n",
      "656939127\n",
      "657079190\n",
      "657081119\n",
      "657648863\n",
      "657786322\n",
      "657891871\n",
      "658020989\n",
      "658024115\n",
      "659743451\n",
      "660065134\n",
      "660510504\n",
      "661744804\n",
      "662351346\n",
      "662960692\n",
      "662989044\n",
      "663488086\n",
      "663870102\n",
      "663876890\n",
      "664394265\n",
      "664414452\n",
      "665726259\n",
      "666274171\n",
      "666563739\n",
      "667014179\n",
      "669239852\n",
      "670398566\n",
      "670721500\n",
      "672675348\n",
      "673173248\n",
      "673475038\n",
      "680601013\n",
      "681673022\n",
      "681674286\n",
      "682732631\n",
      "685494041\n",
      "686919436\n",
      "688678784\n",
      "689402014\n",
      "691201201\n",
      "691208363\n",
      "696717748\n",
      "699154913\n",
      "703731597\n",
      "704826374\n",
      "710938138\n",
      "712923993\n",
      "716646781\n",
      "717214654\n"
     ]
    }
   ],
   "source": [
    "for i in open(\"allendata/natural_scenes/sweep/finished\", \"r\"):\n",
    "    i = i.split()[0]\n",
    "    print(i)\n",
    "    if not path.exists(\"allendata/natural_scenes/response2d/\"+str(i)+\".npy\"):\n",
    "        data = np.load(\"allendata/natural_scenes/sweep/\"+i+\".npy\",allow_pickle=True)\n",
    "        data = truc(data)\n",
    "        np.save(\"allendata/natural_scenes/response3d/\"+str(i)+\".npy\",data)\n",
    "        np.save(\"allendata/natural_scenes/response2d/\"+str(i)+\".npy\",seq2mean(data))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5950, 169)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.load(\"test1.npy\",allow_pickle=True)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63,)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns.sweep_response.iloc[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500964514\n",
      "501559087\n",
      "501567237\n",
      "501794235\n",
      "501889084\n",
      "502254330\n",
      "502368172\n",
      "502382906\n",
      "503772253\n",
      "503866276\n",
      "504508104\n",
      "504625475\n",
      "505314372\n",
      "506278598\n",
      "506520703\n",
      "507304910\n",
      "508378520\n",
      "508546728\n",
      "509962140\n",
      "510166410\n",
      "510345479\n",
      "510517609\n",
      "510698988\n",
      "510699005\n",
      "510705057\n",
      "510706209\n",
      "510712856\n",
      "510938357\n",
      "511458599\n",
      "512124564\n",
      "512149367\n",
      "528480613\n",
      "529487172\n",
      "531342486\n",
      "539291372\n",
      "541048140\n",
      "541206592\n",
      "544507627\n",
      "547573479\n",
      "548227481\n",
      "550197614\n",
      "553000583\n",
      "553012563\n",
      "554021353\n",
      "555018432\n",
      "555257822\n",
      "555813683\n",
      "556338149\n",
      "556919719\n",
      "557182484\n",
      "557420967\n",
      "557956194\n",
      "560806119\n",
      "561528269\n",
      "561994407\n",
      "562220433\n",
      "562222842\n",
      "562296530\n",
      "566716486\n",
      "567446262\n",
      "567734055\n",
      "568753147\n",
      "568775666\n",
      "568796683\n",
      "569407590\n",
      "569722788\n",
      "569810774\n",
      "570006683\n",
      "570059563\n",
      "570080979\n",
      "570472763\n",
      "572489757\n",
      "572499364\n",
      "574415929\n",
      "574685634\n",
      "574824922\n",
      "575890352\n",
      "578220711\n",
      "578260272\n",
      "579966129\n",
      "579968437\n",
      "580124131\n",
      "580570243\n",
      "580631157\n",
      "581676766\n",
      "583296652\n",
      "584930390\n",
      "584983527\n",
      "585953317\n",
      "589098031\n",
      "589637407\n",
      "590109296\n",
      "591414748\n",
      "591563201\n",
      "591640135\n",
      "591823992\n",
      "593240301\n",
      "593389688\n",
      "593646972\n",
      "593695648\n",
      "593887846\n",
      "594314285\n",
      "595899822\n",
      "595904738\n",
      "596824582\n",
      "598905882\n",
      "599420257\n",
      "601260046\n",
      "601338233\n",
      "602170460\n",
      "603226974\n",
      "603516552\n",
      "603552279\n",
      "603863146\n",
      "605683787\n",
      "605913519\n",
      "606031380\n",
      "606151117\n",
      "606873744\n",
      "606960609\n",
      "612566550\n",
      "613062511\n",
      "613074493\n",
      "614535829\n",
      "614851823\n",
      "617395439\n",
      "629789161\n",
      "636924826\n",
      "636930038\n",
      "637123467\n",
      "637670417\n",
      "638754323\n",
      "639117180\n",
      "639252499\n",
      "639437387\n",
      "642651898\n",
      "645035917\n",
      "645040965\n",
      "645687787\n",
      "645695159\n",
      "646291324\n",
      "648389302\n",
      "648644110\n",
      "649398482\n",
      "650509372\n",
      "650510708\n",
      "652336350\n",
      "652337551\n",
      "652345569\n",
      "652988777\n",
      "652989705\n",
      "652990651\n",
      "653122445\n",
      "653922961\n",
      "653924226\n",
      "656939127\n",
      "657079190\n",
      "657081119\n",
      "657648863\n",
      "657786322\n",
      "657891871\n",
      "658020989\n",
      "658024115\n",
      "659743451\n",
      "660065134\n",
      "660510504\n",
      "661744804\n",
      "662351346\n",
      "662960692\n",
      "662989044\n",
      "663488086\n",
      "663870102\n",
      "663876890\n",
      "664394265\n",
      "664414452\n",
      "665726259\n",
      "666274171\n",
      "666563739\n",
      "667014179\n",
      "669239852\n",
      "670398566\n",
      "670721500\n",
      "672675348\n",
      "673173248\n",
      "673475038\n",
      "680601013\n",
      "681673022\n",
      "681674286\n",
      "682732631\n",
      "685494041\n",
      "686919436\n",
      "688678784\n",
      "689402014\n",
      "691201201\n",
      "691208363\n",
      "696717748\n",
      "699154913\n",
      "703731597\n",
      "704826374\n",
      "710938138\n",
      "712923993\n",
      "716646781\n",
      "717214654\n"
     ]
    }
   ],
   "source": [
    "# EXTRACT MODELS by ROIs\n",
    "\n",
    "for i in open(\"allendata/natural_scenes/sweep/finished\", \"r\"):\n",
    "    i = i.split()[0]\n",
    "    print(i)\n",
    "    meta = boc.get_ophys_experiment_data(ophys_experiment_id=int(i))\n",
    "    roi = meta.get_metadata()['targeted_structure']\n",
    "    f = open(\"allendata/natural_scenes/roi/\"+roi+\".txt\", \"a\")\n",
    "    f.write(i+\"\\n\")\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VISl'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = boc.get_ophys_experiment_data(ophys_experiment_id=652989705)\n",
    "meta.get_metadata()['targeted_structure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages/allensdk/brain_observatory/natural_scenes.py:391: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  ns._response = f[\"analysis/response_ns\"].value\n",
      "/Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages/allensdk/brain_observatory/natural_scenes.py:392: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  ns._binned_dx_sp = f[\"analysis/binned_dx_sp\"].value\n",
      "/Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages/allensdk/brain_observatory/natural_scenes.py:393: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  ns._binned_cells_sp = f[\"analysis/binned_cells_sp\"].value\n",
      "/Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages/allensdk/brain_observatory/natural_scenes.py:394: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  ns._binned_dx_vis = f[\"analysis/binned_dx_vis\"].value\n",
      "/Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages/allensdk/brain_observatory/natural_scenes.py:395: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  ns._binned_cells_vis = f[\"analysis/binned_cells_vis\"].value\n",
      "/Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages/allensdk/brain_observatory/natural_scenes.py:398: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  ns.noise_correlation = f[\"analysis/noise_corr_ns\"].value\n",
      "/Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages/allensdk/brain_observatory/natural_scenes.py:400: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  ns.signal_correlation = f[\"analysis/signal_corr_ns\"].value\n",
      "/Users/doerlbh/opt/anaconda3/envs/mlenv37/lib/python3.7/site-packages/allensdk/brain_observatory/natural_scenes.py:402: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  ns.representational_similarity = f[\"analysis/rep_similarity_ns\"].value\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed_max_sp</th>\n",
       "      <th>speed_min_sp</th>\n",
       "      <th>ptest_sp</th>\n",
       "      <th>mod_sp</th>\n",
       "      <th>speed_max_vis</th>\n",
       "      <th>speed_min_vis</th>\n",
       "      <th>ptest_vis</th>\n",
       "      <th>mod_vis</th>\n",
       "      <th>ori_sg</th>\n",
       "      <th>sf_sg</th>\n",
       "      <th>...</th>\n",
       "      <th>genotype</th>\n",
       "      <th>session_start_time</th>\n",
       "      <th>session_type</th>\n",
       "      <th>specimen_name</th>\n",
       "      <th>cre_line</th>\n",
       "      <th>imaging_depth_um</th>\n",
       "      <th>age_days</th>\n",
       "      <th>device</th>\n",
       "      <th>device_name</th>\n",
       "      <th>pipeline_version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.39093</td>\n",
       "      <td>50.2893</td>\n",
       "      <td>4.03113e-06</td>\n",
       "      <td>True</td>\n",
       "      <td>3.36892</td>\n",
       "      <td>49.0587</td>\n",
       "      <td>0.0297926</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>Sst-IRES-Cre/wt;Ai148(TIT2L-GC6f-ICL-tTA2)/wt</td>\n",
       "      <td>2017-05-18 15:06:20</td>\n",
       "      <td>three_session_B</td>\n",
       "      <td>Sst-IRES-Cre;Ai148(CAM)-306500</td>\n",
       "      <td>Sst-IRES-Cre/wt</td>\n",
       "      <td>275</td>\n",
       "      <td>107</td>\n",
       "      <td>Nikon A1R-MP multiphoton microscope</td>\n",
       "      <td>CAM2P.2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.39093</td>\n",
       "      <td>50.2893</td>\n",
       "      <td>2.00157e-06</td>\n",
       "      <td>True</td>\n",
       "      <td>3.36892</td>\n",
       "      <td>49.6965</td>\n",
       "      <td>3.06858e-08</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>Sst-IRES-Cre/wt;Ai148(TIT2L-GC6f-ICL-tTA2)/wt</td>\n",
       "      <td>2017-05-18 15:06:20</td>\n",
       "      <td>three_session_B</td>\n",
       "      <td>Sst-IRES-Cre;Ai148(CAM)-306500</td>\n",
       "      <td>Sst-IRES-Cre/wt</td>\n",
       "      <td>275</td>\n",
       "      <td>107</td>\n",
       "      <td>Nikon A1R-MP multiphoton microscope</td>\n",
       "      <td>CAM2P.2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.493</td>\n",
       "      <td>50.2893</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>13.8216</td>\n",
       "      <td>46.0235</td>\n",
       "      <td>0.000120555</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>Sst-IRES-Cre/wt;Ai148(TIT2L-GC6f-ICL-tTA2)/wt</td>\n",
       "      <td>2017-05-18 15:06:20</td>\n",
       "      <td>three_session_B</td>\n",
       "      <td>Sst-IRES-Cre;Ai148(CAM)-306500</td>\n",
       "      <td>Sst-IRES-Cre/wt</td>\n",
       "      <td>275</td>\n",
       "      <td>107</td>\n",
       "      <td>Nikon A1R-MP multiphoton microscope</td>\n",
       "      <td>CAM2P.2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39.8526</td>\n",
       "      <td>5.39093</td>\n",
       "      <td>0.0146844</td>\n",
       "      <td>True</td>\n",
       "      <td>53.0654</td>\n",
       "      <td>3.36892</td>\n",
       "      <td>8.27394e-07</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>Sst-IRES-Cre/wt;Ai148(TIT2L-GC6f-ICL-tTA2)/wt</td>\n",
       "      <td>2017-05-18 15:06:20</td>\n",
       "      <td>three_session_B</td>\n",
       "      <td>Sst-IRES-Cre;Ai148(CAM)-306500</td>\n",
       "      <td>Sst-IRES-Cre/wt</td>\n",
       "      <td>275</td>\n",
       "      <td>107</td>\n",
       "      <td>Nikon A1R-MP multiphoton microscope</td>\n",
       "      <td>CAM2P.2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.39093</td>\n",
       "      <td>50.2893</td>\n",
       "      <td>0.00268292</td>\n",
       "      <td>True</td>\n",
       "      <td>26.2198</td>\n",
       "      <td>44.7929</td>\n",
       "      <td>0.943861</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Sst-IRES-Cre/wt;Ai148(TIT2L-GC6f-ICL-tTA2)/wt</td>\n",
       "      <td>2017-05-18 15:06:20</td>\n",
       "      <td>three_session_B</td>\n",
       "      <td>Sst-IRES-Cre;Ai148(CAM)-306500</td>\n",
       "      <td>Sst-IRES-Cre/wt</td>\n",
       "      <td>275</td>\n",
       "      <td>107</td>\n",
       "      <td>Nikon A1R-MP multiphoton microscope</td>\n",
       "      <td>CAM2P.2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  speed_max_sp speed_min_sp     ptest_sp mod_sp speed_max_vis speed_min_vis  \\\n",
       "0      5.39093      50.2893  4.03113e-06   True       3.36892       49.0587   \n",
       "1      5.39093      50.2893  2.00157e-06   True       3.36892       49.6965   \n",
       "2       17.493      50.2893            1   True       13.8216       46.0235   \n",
       "3      39.8526      5.39093    0.0146844   True       53.0654       3.36892   \n",
       "4      5.39093      50.2893   0.00268292   True       26.2198       44.7929   \n",
       "\n",
       "     ptest_vis mod_vis ori_sg sf_sg  ...  \\\n",
       "0    0.0297926    True      5     5  ...   \n",
       "1  3.06858e-08    True      0     2  ...   \n",
       "2  0.000120555    True      5     3  ...   \n",
       "3  8.27394e-07    True      5     2  ...   \n",
       "4     0.943861    True      1     1  ...   \n",
       "\n",
       "                                        genotype  session_start_time  \\\n",
       "0  Sst-IRES-Cre/wt;Ai148(TIT2L-GC6f-ICL-tTA2)/wt 2017-05-18 15:06:20   \n",
       "1  Sst-IRES-Cre/wt;Ai148(TIT2L-GC6f-ICL-tTA2)/wt 2017-05-18 15:06:20   \n",
       "2  Sst-IRES-Cre/wt;Ai148(TIT2L-GC6f-ICL-tTA2)/wt 2017-05-18 15:06:20   \n",
       "3  Sst-IRES-Cre/wt;Ai148(TIT2L-GC6f-ICL-tTA2)/wt 2017-05-18 15:06:20   \n",
       "4  Sst-IRES-Cre/wt;Ai148(TIT2L-GC6f-ICL-tTA2)/wt 2017-05-18 15:06:20   \n",
       "\n",
       "      session_type                   specimen_name         cre_line  \\\n",
       "0  three_session_B  Sst-IRES-Cre;Ai148(CAM)-306500  Sst-IRES-Cre/wt   \n",
       "1  three_session_B  Sst-IRES-Cre;Ai148(CAM)-306500  Sst-IRES-Cre/wt   \n",
       "2  three_session_B  Sst-IRES-Cre;Ai148(CAM)-306500  Sst-IRES-Cre/wt   \n",
       "3  three_session_B  Sst-IRES-Cre;Ai148(CAM)-306500  Sst-IRES-Cre/wt   \n",
       "4  three_session_B  Sst-IRES-Cre;Ai148(CAM)-306500  Sst-IRES-Cre/wt   \n",
       "\n",
       "  imaging_depth_um  age_days                               device device_name  \\\n",
       "0              275       107  Nikon A1R-MP multiphoton microscope     CAM2P.2   \n",
       "1              275       107  Nikon A1R-MP multiphoton microscope     CAM2P.2   \n",
       "2              275       107  Nikon A1R-MP multiphoton microscope     CAM2P.2   \n",
       "3              275       107  Nikon A1R-MP multiphoton microscope     CAM2P.2   \n",
       "4              275       107  Nikon A1R-MP multiphoton microscope     CAM2P.2   \n",
       "\n",
       "  pipeline_version  \n",
       "0              3.0  \n",
       "1              3.0  \n",
       "2              3.0  \n",
       "3              3.0  \n",
       "4              3.0  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns = boc.get_ophys_experiment_analysis(ophys_experiment_id=exp_ses_id, stimulus_type='natural_scenes')\n",
    "ns.peak.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns.representational_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visual_areas = ['VISal', 'VISam', 'VISl', 'VISp', 'VISpm', 'VISrl']\n",
    "cre_line = 'Cux2-CreERT2'\n",
    "exps = boc.get_experiment_containers(targeted_structures=visual_areas, cre_lines=[cre_line])\n",
    "data_set = pd.DataFrame(exps)\n",
    "\n",
    "# ses_set = []\n",
    "# ses_rdm = []\n",
    "for i,exp_id in enumerate(data_set['id']):\n",
    "    print(i,exp_id)\n",
    "    exp_obj_info = pd.DataFrame(boc.get_ophys_experiments(experiment_container_ids=[exp_id]))\n",
    "    exp_ses_id = boc.get_ophys_experiments(experiment_container_ids=[exp_id], stimuli=['natural_scenes'])[0]['id']\n",
    "    ses_obj = boc.get_ophys_experiment_data(ophys_experiment_id=exp_ses_id)    \n",
    "    ns = boc.get_ophys_experiment_analysis(ophys_experiment_id=exp_ses_id, stimulus_type='natural_scenes')\n",
    "    ses_set.append(ses_obj)\n",
    "    ses_rdm.append(ns.representational_similarity)\n",
    "\n",
    "allen_data = { 'meta': ses_set , 'rdms' : ses_rdm }\n",
    "pk.dump( allen_data, open( \"brain_observatory/allen_data.pkl\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = len(ses_set)\n",
    "model_names = [ses_set[i].get_metadata()['targeted_structure'] for i in range(n_models)]\n",
    "measurement_model = [ses_set[i].get_metadata()['imaging_depth_um'] for i in range(n_models)]\n",
    "rdms_array = np.array([ses_rdm[i] for i in range(n_models)])\n",
    "\n",
    "allen_data = { 'n_models':n_models, 'model_names':model_names, 'measurement_model':measurement_model, 'rdms_array':rdms_array }\n",
    "\n",
    "pk.dump( allen_data, open( \"brain_observatory/allen_data.pkl\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allen_data = pk.load( open( \"brain_observatory/allen_data.pkl\", \"rb\" ) )\n",
    "\n",
    "n_models = allen_data['n_models']\n",
    "model_names = allen_data['model_names']\n",
    "measurement_model = allen_data['measurement_model']\n",
    "rdms_array = allen_data['rdms_array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdms_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These steps are not specific to the toolbox, but to the format the RDMs were originally saved in.\n",
    "To load other data, simply transform them such that they are numpy arrays of either the whole RDM or vector format of the upper triangular part of the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the model RDMs as a pyRSA object\n",
    "We place the RDMs in a pyRSA object which can contain additional descriptors for the RDMs and the experimental conditions.\n",
    "Here we label each RDM with the name of the brain-computational model (AlexNet layer) and the name of the measurement model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rdms = pyrsa.rdm.RDMs(rdms_array,\n",
    "                            rdm_descriptors={'brain_computational_model':model_names,\n",
    "                                             'measurement_model':measurement_model},\n",
    "                            dissimilarity_measure='Euclidean'\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rdms.dissimilarities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The variable model_rdms is now a custom object, which contains all the RDMs from the .mat file with the additional information.\n",
    "It also has a few methods for forming subsets of the data, saving and loading, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the RDMs from AlexNet layer conv1\n",
    "\n",
    "As a simple example, select the RDMs that correspond to the first convolutional layer. These can then be plotted using the function pyrsa.vis.show_rdm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_rdms = model_rdms.subset('brain_computational_model','VISl')\n",
    "plt.figure(figsize=(10,10))\n",
    "pyrsa.vis.show_rdm(conv1_rdms, do_rank_transform=True, rdm_descriptor='measurement_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the RDMs which were generated from convolutional layer 1 by different measurement models. Each RDM is labeled with the name of the measurement model. Also in the lower right corner the average RDM is plotted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print information about a set of RDMs\n",
    "The pyRSA objects can simply be passed to the print function to obtain a short description of their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visl_rdms = model_rdms.subset('brain_computational_model','VISl')\n",
    "print(visl_rdms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "Of course, you can also show all RDMs or select any other subset. Have a look at the different RDMs!\n",
    "\n",
    "How many RDMs are there for each layer?\n",
    "\n",
    "Generate a plot which shows all RDMs with the 'complete' measurement model.\n",
    "\n",
    "How different do the different measurement models look to you and how different do the different layers look?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Fixed model inference\n",
    "### Load data RDMs\n",
    "Here we use simulated data to demonstrate RSA inference.\n",
    "Since we know the true data-generating model in each case, we can tell when inference fails or succeeds.\n",
    "\n",
    "For each data RDM, we obtain the name of the underlying Layer, a full width at half maximum (FWHM) value and a noise standard deviation. The FWHM value specifies the spatial range the simulated voxels average over. The noise standard deviation specifies how much noise was added to the voxel responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# matlab_data = io.matlab.loadmat('rdms_inferring/noisyModelRDMs_A2020.mat')\n",
    "# repr_names_matlab = matlab_data['reprNames']\n",
    "# fwhms_matlab = matlab_data['FWHMs']\n",
    "# noise_std_matlab = matlab_data['relNoiseStds']\n",
    "# rdms_matlab = matlab_data['noisyModelRDMs']\n",
    "# repr_names = [repr_names_matlab[i][0][0] for i in range(repr_names_matlab.shape[0])]\n",
    "# fwhms = fwhms_matlab.squeeze().astype('float')\n",
    "# noise_std = noise_std_matlab.squeeze().astype('float')\n",
    "# rdms_matrix = rdms_matlab.squeeze().astype('float')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the data RDMs for inference\n",
    "\n",
    "Here we choose which data RDMs we use for the exercise. You can change the representation, the noise level and the amount of averaging by chaning the index values at the beginning.\n",
    "\n",
    "We then convert the chosen data RDMs into an pyrsa RDMs object and display them as we did for the model RDMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices choosing brain-computational model, noise level, and the size of the kernel with which each voxel samples the neural activity\n",
    "\n",
    "rdms_data = model_rdms.subset('brain_computational_model','VISam')\n",
    "repr_name = 'VISam'\n",
    "noise_level = 0.5\n",
    "rdms_data.dissimilarities = rdms_data.dissimilarities + np.random.normal(0.0, noise_level, size=rdms_data.dissimilarities.shape)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "pyrsa.vis.show_rdm(rdms_data, do_rank_transform=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define fixed models\n",
    "An \"RDM model\" is a pyRSA object that can predict a data RDM.\n",
    "For example, a flexible RDM model may contain a set of predictor RDMs, which predict the data RDM as a weighted combination.\n",
    "Here we use fixed RDM models, which contain just a single RDM with no parameters to be fitted.\n",
    "\n",
    "Models are generated by first choosing the RDM, in this case the one with the right \"brain_computational_model\". This object is then passed to the function `pyrsa.model.ModelFixed`, which generates a fixed RDM model. These RDM models are then collected in the list `models`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i_model in np.unique(model_names):\n",
    "    rdm_m = model_rdms.subset('brain_computational_model', i_model)\n",
    "    m = pyrsa.model.ModelFixed(i_model, rdm_m)\n",
    "    models.append(m)\n",
    "\n",
    "print('created the following models:')\n",
    "for i in range(len(models)):\n",
    "    print(models[i].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare model RDMs to measured RDMs\n",
    "Evaluate models naively, i.e. simply compute the average correlation to the data RDMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1 = pyrsa.inference.eval_fixed(models, rdms_data, method='corr')\n",
    "pyrsa.vis.plot_model_comparison(results_1)\n",
    "\n",
    "#results_1 = pyrsa.inference.eval_fixed(models, rdms_data, method='spearman')\n",
    "#pyrsa.vis.plot_model_comparison(results_1)\n",
    "\n",
    "#results_1 = pyrsa.inference.eval_fixed(models, rdms_data, method='tau-a')\n",
    "#pyrsa.vis.plot_model_comparison(results_1)\n",
    "\n",
    "#results_1 = pyrsa.inference.eval_fixed(models, rdms_data, method='rho-a')\n",
    "#pyrsa.vis.plot_model_comparison(results_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these plots the models do not have errorbars as we did not run any estimate of the variablitiy.\n",
    "The upper bound of the noise ceiling is computed by finding the RDM with the highest possible average similarity to the measured RDMs. This is not 1 because the RDMs for different subjects or measurements differ. The lower bound of the noise ceiling is a leave one out crossvalidation of this averaging procedure, i.e. we find the RDM to perform optimally on all but one of the RDMs and evaluate this average RDM on the left-out RDM. Each RDM is left out once and the correlations are averaged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping\n",
    "To perform statistical comparisons and estimate how uncertain we should be about the models' performance, we can perform bootstrapping:\n",
    "\n",
    "In each plot the errobars correspond to +/- one SEM based on the bootrap samples.\n",
    "The lines above the plot show which pairwise comparisons are significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model comparison by bootstrapping the subjects\n",
    "We can bootstrap resample the subjects, which estimates how variable the model performances would be if we repeted the experiment with the same stimuli but new subjects from the same population. Based on that uncertainty estimate, we can statistically compare model performances. We would like to take the many pairwise model comparisons into account in performing inference. We have a choice: We can either control the family wise error rate (FWER) or the false discovery rate (FDR). Here we use a Bonferroni correction for FWER and the Benjamini-Hochberg procedure for FDR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2a = pyrsa.inference.eval_bootstrap_rdm(models, rdms_data, method='corr')\n",
    "pyrsa.vis.plot_model_comparison(results_2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model comparison by bootstrapping the stimuli\n",
    "We can alternatively bootstrap resample the stimuli to estimate how much model performance would vary if we repeated the experiment with the same subjects using a new sample of stimuli from the same population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2b = pyrsa.inference.eval_bootstrap_pattern(models, rdms_data, method='corr')\n",
    "pyrsa.vis.plot_model_comparison(results_2b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model comparison by bootstrapping both stimuli and subjects\n",
    "Finally, we can bootstrap resample both stimuli and subjects to estimate how variable the model performances would be if we repeated the experiment with new subjects and new stimuli from their respective populations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "results_2c = pyrsa.inference.eval_bootstrap(models, rdms_data, method='corr')\n",
    "pyrsa.vis.plot_model_comparison(results_2c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "Does the right model win? And do the mean estimates from bootstrapping differ from the evaluations over the whole dataset?\n",
    "\n",
    "Compare the results for the different bootstrapping methods. Which method leads to the widest confidence intervals, which one to the smallest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Crossvalidation for flexible models\n",
    "### Defining flexible models\n",
    "Here we use a type of flexible model called a *selection model*. This type of model specifies that the true RDM is one from a list of RDMs. To evaluate flexible models, they have to be fitted to data, i.e. we need to provide some data, which can be used to adjust the RDM-prediction of the model. For a selection model, the fitting process simply selects the RDM that performs best on the training data. The model will perform better on this data than on independent data. An unbiased performance estimate therefore requires independent test data. Crossvalidation is a data-efficient way of obtaining an unbiased performance estimate.\n",
    "\n",
    "We first have to generate the selection models. This process is the same as for fixed models, but uses `pyrsa.model.ModelSelect` and passes multiple RDMs instead of a single one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_flex = []\n",
    "for i_model in np.unique(model_names):\n",
    "    models_flex.append(pyrsa.model.ModelSelect(i_model,\n",
    "        model_rdms.subset('brain_computational_model', i_model)))\n",
    "\n",
    "print('created the following models:')\n",
    "for i in range(len(models_flex)):\n",
    "    print(models_flex[i].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossvalidation\n",
    "As a first step, we split our data into training and test sets, which should not share either subjects or stimuli. To do so, we split each dimension into k groups and leave one of these groups out as a testset and use all others as training data. Models choose their parameters to maximize performance on the training set and are evaluated on the test set. Additionally a so-called *ceil set* is created, which contains the data from the training subjects for the test stimuli, which is necessary for calculating a noise ceiling.\n",
    "\n",
    "The variables `k_pattern` and `k_rdm` specify how many folds should be formed over stimuli and subjects, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set, ceil_set = pyrsa.inference.sets_k_fold(rdms_data, k_pattern=3, k_rdm=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these sets we can now evaluate our models, as we did without crossvalidaton, and plot the results. The performance estimates will be averaged across folds and we obtain a single performance estimate without errorbars. The variability over cross-validation folds are not indicative of the variability across independent datasets. Although training and test data are independent of each other in each fold, performance estimates are not independent across folds of crossvalidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_3_cv = pyrsa.inference.crossval(models_flex, rdms_data, train_set, test_set,\n",
    "                                        ceil_set=ceil_set, method='corr')\n",
    "# plot results\n",
    "pyrsa.vis.plot_model_comparison(results_3_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapped Crossvalidation\n",
    "\n",
    "We can perform bootstrapping around the crossvalidation to get uncertainty estimates for the evaluation and for model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_3_full = pyrsa.inference.bootstrap_crossval(models_flex, rdms_data, k_pattern=4, k_rdm=2, method='corr', N=100)\n",
    "# plot results\n",
    "pyrsa.vis.plot_model_comparison(results_3_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the right model win?\n",
    "\n",
    "Try some different settings for the crossvalidation: How do the results change when you make the training and test sets larger or smaller?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[your code here]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv37",
   "language": "python",
   "name": "mlenv37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
