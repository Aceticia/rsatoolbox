{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: RSA 1.0 inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import io\n",
    "import pyrsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model RDMs\n",
    "Here the models are different layers of Alexnet.\n",
    "For each layer, different models of how the fMRI voxels sample the neurons are being considered.\n",
    "\n",
    "The simulated data are from a Matlab program (Kriegeskorte & Diedrichsen 2016), so we load the Matlab files in .mat format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matlab_data = io.matlab.loadmat('rdms_inferring/modelRDMs.mat')\n",
    "matlab_data = matlab_data['modelRDMs']\n",
    "n_models = len(matlab_data[0])\n",
    "model_names = [matlab_data[0][i][0][0] for i in range(n_models)]\n",
    "measurement_model = [matlab_data[0][i][1][0] for i in range(n_models)]\n",
    "rdms_array = np.array([matlab_data[0][i][2][0] for i in range(n_models)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the model RDMs as a pyRSA object\n",
    "We place the RDMs in a pyRSA object which can contain additional descriptors for the RDMs and the experimental conditions.\n",
    "Here we label each RDM with the name of the brain-computational model (AlexNet layer) and the name of the measurement model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rdms = pyrsa.rdm.RDMs(rdms_array,\n",
    "                            rdm_descriptors={'brain_computational_model':model_names,\n",
    "                                             'measurement_model':measurement_model}\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the RDMs from AlexNet layer conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_rdms = model_rdms.subset('brain_computational_model','conv1')\n",
    "pyrsa.vis.show_rdm(conv1_rdms, do_rank_transform=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print info about a set of RDMs\n",
    "The pyRSA objects can simply be passed to the print function to obtain a short description of their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conv1_rdms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data RDMs\n",
    "Here we use simulated data to demonstrate RSA inference.\n",
    "Since we know the true data-generating model in each case, we can tell when inference fails or succeeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matlab_data = io.matlab.loadmat('rdms_inferring/noisyModelRDMs_demo.mat')\n",
    "repr_names_matlab = matlab_data['reprNames']\n",
    "fwhms_matlab = matlab_data['FWHMs']\n",
    "noise_std_matlab = matlab_data['relNoiseStds']\n",
    "rdms_matlab = matlab_data['noisyModelRDMs']\n",
    "repr_names = [repr_names_matlab[i][0][0] for i in range(repr_names_matlab.shape[0])]\n",
    "fwhms = fwhms_matlab.squeeze().astype('float')\n",
    "noise_std = noise_std_matlab.squeeze().astype('float')\n",
    "rdms_matrix = rdms_matlab.squeeze().astype('float')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose one of the data RDMs for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices choosing brain-computational model, noise level, and the size of the kernel with which each voxel samples the neural activity\n",
    "i_rep = np.random.randint(len(repr_names)) \n",
    "i_noise = np.random.randint(len(noise_std))\n",
    "i_fwhm = np.random.randint(len(fwhms))\n",
    "\n",
    "# print the chosen representation definition\n",
    "repr_name = repr_names[i_rep]\n",
    "print('The chosen ground truth model is:')\n",
    "print(repr_name)\n",
    "print('with noise level:')\n",
    "print(noise_std[i_noise])\n",
    "print('with averaging width (full width at half magnitude):')\n",
    "print(fwhms[i_fwhm])\n",
    "\n",
    "# put the rdms into an RDMs object and show it\n",
    "rdms_data = pyrsa.rdm.RDMs(rdms_matrix[:, i_rep, i_fwhm, i_noise, :].transpose())\n",
    "pyrsa.vis.show_rdm(rdms_data, do_rank_transform=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define fixed models\n",
    "An \"RDM model\" is a pyRSA object that can predict a data RDM.\n",
    "For example, an RDM model may contain a set of predictor RDMs, which predict the data RDM as a weighted combination.\n",
    "Here we use fixed RDM models, which contain just a single RDM with no parameters to be fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i_model in np.unique(model_names):\n",
    "    models.append(pyrsa.model.ModelFixed(i_model,\n",
    "        model_rdms.subset('brain_computational_model', i_model).subset('measurement_model','complete')))\n",
    "\n",
    "print('created the following models:')\n",
    "for i in range(len(models)):\n",
    "    print(models[i].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSA 1.0: Just compare model RDMs to measured RDMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate models naively, i.e. simply compute the average correlation to the simulated data\n",
    "results_1 = pyrsa.inference.eval_fixed(models, rdms_data, method='corr')\n",
    "pyrsa.vis.plot_model_comparison(results_1)\n",
    "\n",
    "results_1 = pyrsa.inference.eval_fixed(models, rdms_data, method='spearman')\n",
    "pyrsa.vis.plot_model_comparison(results_1)\n",
    "\n",
    "results_1 = pyrsa.inference.eval_fixed(models, rdms_data, method='tau-a')\n",
    "pyrsa.vis.plot_model_comparison(results_1)\n",
    "\n",
    "# In these plots the models do not have errorbars as we did not run any bootstrapping\n",
    "# The upper noise ceiling is computed by evaluating the average of all rdms, which is \n",
    "# a true upper limit on performance\n",
    "# The lower noise ceiling is a leave one out crossvalidation of the average, i.e. \n",
    "# all but one rdm are averaged and evaluated on the left out rdm.\n",
    "\n",
    "# Make two additional observations here:\n",
    "# 1) The true model is not necessarily winning as the different measurement models yield substantially different RDMs\n",
    "# 2) Often none of the models reaches the noise ceiling\n",
    "# Thus, to make this a sensible analysis one should take the measurement model into account!\n",
    "# (See Kriegeskorte & Diedrichsen 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison by bootstrapping the subjects\n",
    "We can bootstrap resample the subjects to obtain 95% confidence intervals and to perform model-comparative inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2a = pyrsa.inference.eval_bootstrap_rdm(models, rdms_data, method='corr')\n",
    "pyrsa.vis.plot_model_comparison(results_2a, plot_pair_tests=True)\n",
    "\n",
    "#results_2a_spearman = pyrsa.inference.eval_bootstrap_rdm(models, rdms_data, method='spearman')\n",
    "#pyrsa.vis.plot_model_comparison(results_2a_spearman, plot_pair_tests=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison by bootstrapping the stimuli\n",
    "We can bootstrap resample the stimuli to obtain 95% confidence intervals and to perform model-comparative inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) using only bootstrapping over patterns\n",
    "results_2b = pyrsa.inference.eval_bootstrap_pattern(models, rdms_data, method='corr')\n",
    "pyrsa.vis.plot_model_comparison(results_2b)\n",
    "\n",
    "#results_2b_spearman = pyrsa.inference.eval_bootstrap_pattern(models, rdms_data, method='spearman')\n",
    "#pyrsa.vis.plot_model_comparison(results_2b_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# c) bootstrap over both patterns and subjects\n",
    "results_2c = pyrsa.inference.eval_bootstrap(models, rdms_data, method='corr')\n",
    "pyrsa.vis.plot_model_comparison(results_2c)\n",
    "\n",
    "results_2c_spearman = pyrsa.inference.eval_bootstrap(models, rdms_data, method='spearman')\n",
    "pyrsa.vis.plot_model_comparison(results_2c_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RSA 3.0 bootstrapped cross-validation, allows flexible models\n",
    "\n",
    "# defining flexible models, here selection models, i.e. each model layer gets a list of rdms to choose from\n",
    "\n",
    "models_flex = []\n",
    "for i_model in np.unique(model_names):\n",
    "    models_flex.append(pyrsa.model.ModelSelect(i_model,\n",
    "        model_rdms.subset('model', i_model)))\n",
    "\n",
    "print('created the following models:')\n",
    "for i in range(len(models_flex)):\n",
    "    print(models_flex[i].name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we are now using flexible models, we have to do crossvalidation to get an estimate\n",
    "# how well the model would do on new unseen data\n",
    "# generate crossvalidation sets:\n",
    "train_set, test_set, ceil_set = pyrsa.inference.sets_k_fold(rdms_data, k_pattern=3, k_rdm=2)\n",
    "# perform crossvalidation\n",
    "results_3_cv = pyrsa.inference.crossval(models_flex, rdms_data, train_set, test_set, method='corr')\n",
    "# plot results\n",
    "pyrsa.vis.plot_model_comparison(results_3_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To perform bootstrapping around this we can run:\n",
    "results_3_full = pyrsa.inference.bootstrap_crossval(models, rdms_data, k_pattern=3, k_rdm=2, method='corr')\n",
    "# plot results\n",
    "pyrsa.vis.plot_model_comparison(results_3_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
