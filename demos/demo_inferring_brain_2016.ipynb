{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "import pyrsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading RDMs for different layers of Alexnet and different measurement models\n",
    "matlab_data = io.matlab.loadmat('rdms_inferring/modelRDMs.mat')\n",
    "matlab_data = matlab_data['modelRDMs']\n",
    "n_models = len(matlab_data[0])\n",
    "model_names = [matlab_data[0][i][0][0] for i in range(n_models)]\n",
    "measurement_model = [matlab_data[0][i][1][0] for i in range(n_models)]\n",
    "rdms_array = np.array([matlab_data[0][i][2][0] for i in range(n_models)])\n",
    "model_rdms = pyrsa.rdm.RDMs(rdms_array,\n",
    "                            rdm_descriptors={'model':model_names,\n",
    "                                             'measurement':measurement_model}\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as an example show the rdms from layer conv1\n",
    "conv1_rdms = model_rdms.subset('model','conv1')\n",
    "pyrsa.vis.show_rdm(conv1_rdms, do_rank_transform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading simulated data\n",
    "matlab_data = io.matlab.loadmat('rdms_inferring/noisyModelRDMs_A3.mat')\n",
    "repr_names_matlab = matlab_data['reprNames']\n",
    "fwhms_matlab = matlab_data['FWHMs']\n",
    "noise_std_matlab = matlab_data['relNoiseStds']\n",
    "rdms_matlab = matlab_data['noisyModelRDMs']\n",
    "repr_names = [repr_names_matlab[i][0][0] for i in range(repr_names_matlab.shape[0])]\n",
    "fwhms = fwhms_matlab.squeeze().astype('float')\n",
    "noise_std = noise_std_matlab.squeeze().astype('float')\n",
    "rdms_matrix = rdms_matlab.squeeze().astype('float')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly choosing ground truth\n",
    "i_rep = np.random.randint(len(repr_names))\n",
    "i_noise = np.random.randint(len(noise_std))\n",
    "i_fwhm = np.random.randint(len(fwhms))\n",
    "\n",
    "# print the chosen representation definition\n",
    "repr_name = repr_names[i_rep]\n",
    "print('The chosen ground truth model is:')\n",
    "print(repr_name)\n",
    "print('with noise level:')\n",
    "print(noise_std[i_noise])\n",
    "print('with averaging width (full width at half magnitude):')\n",
    "print(fwhms[i_fwhm])\n",
    "\n",
    "# put the rdms into a RDMs object and show it\n",
    "rdms_data = pyrsa.rdm.RDMs(rdms_matrix[:, i_rep, i_fwhm, i_noise, :].transpose())\n",
    "pyrsa.vis.show_rdm(rdms_data, do_rank_transform=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define (fixed) models:\n",
    "models = []\n",
    "for i_model in np.unique(model_names):\n",
    "    models.append(pyrsa.model.ModelFixed(i_model,\n",
    "        model_rdms.subset('model', i_model).subset('measurement','complete')))\n",
    "\n",
    "print('created the following models:')\n",
    "for i in range(len(models)):\n",
    "    print(models[i].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RSA 1.0 just comparing RDMs to the measured RDMs\n",
    "\n",
    "# evaluate models naively, i.e. simply compute the average correlation to the simulated data\n",
    "results_1 = pyrsa.inference.eval_fixed(models, rdms_data, method='corr')\n",
    "pyrsa.vis.plot_model_comparison(results_1)\n",
    "\n",
    "results_1 = pyrsa.inference.eval_fixed(models, rdms_data, method='spearman')\n",
    "pyrsa.vis.plot_model_comparison(results_1)\n",
    "\n",
    "# In these plots the models do not have errorbars as we did not run any bootstrapping\n",
    "# The upper noise ceiling is computed by evaluating the average of all rdms, which is \n",
    "# a true upper limit on performance\n",
    "# The lower noise ceiling is a leave one out crossvalidation of the average, i.e. \n",
    "# all but one rdm are averaged and evaluated on the left out rdm.\n",
    "\n",
    "# Make two additional observations here:\n",
    "# 1) The true model is not necessarily winning as the different measurement models yield substantially different RDMs\n",
    "# 2) Often none of the models reaches the noise ceiling\n",
    "# Thus, to make this a sensible analysis one should take the measurement model into account!\n",
    "# (See Kriegeskorte & Diedrichsen 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RSA 2.0 bootstrapping -> Inference for fixed models with errorbars\n",
    "\n",
    "# performing the same analysis as in RSA 1.0 with bootstrapping based errobars.\n",
    "# a) using only bootstrapping over subjects\n",
    "results_2a = pyrsa.inference.eval_bootstrap_rdm(models, rdms_data, method='corr')\n",
    "pyrsa.vis.plot_model_comparison(results_2a)\n",
    "\n",
    "results_2a_spearman = pyrsa.inference.eval_bootstrap_rdm(models, rdms_data, method='spearman')\n",
    "pyrsa.vis.plot_model_comparison(results_2a_spearman)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) using only bootstrapping over patterns\n",
    "results_2b = pyrsa.inference.eval_bootstrap_pattern(models, rdms_data, method='corr')\n",
    "pyrsa.vis.plot_model_comparison(results_2b)\n",
    "\n",
    "results_2b_spearman = pyrsa.inference.eval_bootstrap_pattern(models, rdms_data, method='spearman')\n",
    "pyrsa.vis.plot_model_comparison(results_2b_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) bootstrap over both patterns and subjects\n",
    "results_2c = pyrsa.inference.eval_bootstrap(models, rdms_data, method='corr')\n",
    "pyrsa.vis.plot_model_comparison(results_2c)\n",
    "\n",
    "results_2c_spearman = pyrsa.inference.eval_bootstrap(models, rdms_data, method='spearman')\n",
    "pyrsa.vis.plot_model_comparison(results_2c_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RSA 3.0 bootstrapped cross-validation, allows flexible models\n",
    "\n",
    "# defining flexible models, here selection models, i.e. each model layer gets a list of rdms to choose from\n",
    "\n",
    "models_flex = []\n",
    "for i_model in np.unique(model_names):\n",
    "    models_flex.append(pyrsa.model.ModelSelect(i_model,\n",
    "        model_rdms.subset('model', i_model)))\n",
    "\n",
    "print('created the following models:')\n",
    "for i in range(len(models_flex)):\n",
    "    print(models_flex[i].name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we are now using flexible models, we have to do crossvalidation to get an estimate\n",
    "# how well the model would do on new unseen data\n",
    "# generate crossvalidation sets:\n",
    "train_set, test_set, ceil_set = pyrsa.inference.sets_k_fold(rdms_data, k_pattern=3, k_rdm=2)\n",
    "# perform crossvalidation\n",
    "results_3_cv = pyrsa.inference.crossval(models_flex, rdms_data, train_set, test_set, method='corr')\n",
    "# plot results\n",
    "pyrsa.vis.plot_model_comparison(results_3_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To perform bootstrapping around this we can run:\n",
    "results_3_full = pyrsa.inference.bootstrap_crossval(models, rdms_data, k_pattern=3, k_rdm=2, method='corr')\n",
    "# plot results\n",
    "pyrsa.vis.plot_model_comparison(results_3_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
